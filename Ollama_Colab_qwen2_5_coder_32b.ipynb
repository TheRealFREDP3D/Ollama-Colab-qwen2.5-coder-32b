{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vskOpJN65t5"
   },
   "source": [
    "# Ollama on Colab - qwen2.5-coder-32b\n",
    "\n",
    "# =============================================\n",
    "# IMPORTANT: How to get your ngrok authtoken\n",
    "# =============================================\n",
    "# 1. Go to https://ngrok.com and sign up for a free account\n",
    "# 2. After signing in, go to https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "# 3. Copy your authtoken\n",
    "# 4. In Google Colab:\n",
    "#    - Click on the key icon in the left sidebar to open \"Secrets\"\n",
    "#    - Click \"Add new secret\"\n",
    "#    - Set \"Name\" as: authtoken\n",
    "#    - Set \"Value\" as: your-ngrok-token-here\n",
    "#    - Click \"Add\"\n",
    "# 5. Run this notebook with a GPU runtime (Runtime -> Change runtime type -> GPU)\n",
    "# ============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF_hVkoxLGPm"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNBgSq6633TW",
    "outputId": "d31119fa-ec1d-4c10-dece-07bd6c816e03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "collapsed": true,
    "id": "A_SFH8TxVt4d",
    "outputId": "9200de01-2fb4-4121-894e-13848045da8e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2hRFFRiub4do3XOPSTIj5RHc232_2MNnfhnysxZsVEHmdGddp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "userdata.get('authtoken')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssawyk7dLv3t"
   },
   "source": [
    "Here is the code that will:\n",
    "\n",
    "- Install Ollama\n",
    "- Setup ngrok to acces your Ollama server from outside of Colab\n",
    "- Download the **qwen2.5-coder-32b** LLM model.\n",
    "- Be patient, it's a 20GB model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "U7CFRX1Sh6g9",
    "outputId": "b9c6dec4-a544-495d-a627-95dae0ea645d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> starting ollama serve\n",
      ">>> starting ollama pull qwen2.5-coder:32b\n",
      ">>> starting ngrok http --log stderr 11434 --host-header=\"localhost:11434\"\n",
      "2024/11/20 01:11:53 routes.go:1189: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2024-11-20T01:11:53.088Z level=INFO source=images.go:755 msg=\"total blobs: 5\"\n",
      "time=2024-11-20T01:11:53.088Z level=INFO source=images.go:762 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-11-20T01:11:53.089Z level=INFO source=routes.go:1240 msg=\"Listening on 127.0.0.1:11434 (version 0.4.2)\"\n",
      "time=2024-11-20T01:11:53.089Z level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama1983770019/runners\n",
      "t=2024-11-20T01:11:53+0000 lvl=info msg=\"no configuration paths supplied\"\n",
      "t=2024-11-20T01:11:53+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
      "t=2024-11-20T01:11:53+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
      "t=2024-11-20T01:11:53+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
      "time=2024-11-20T01:11:53.685Z level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[cpu cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm]\"\n",
      "time=2024-11-20T01:11:53.686Z level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\n",
      "time=2024-11-20T01:11:53.909Z level=INFO source=types.go:123 msg=\"inference compute\" id=GPU-6ae09678-7863-d645-765e-eb526e07884f library=cuda variant=v12 compute=7.5 driver=12.2 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n",
      "[GIN] 2024/11/20 - 01:11:53 | 200 |      70.105µs |       127.0.0.1 | HEAD     \"/\"\n",
      "t=2024-11-20T01:11:54+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
      "t=2024-11-20T01:11:54+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
      "t=2024-11-20T01:11:54+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://1f13-34-142-142-235.ngrok-free.app\n",
      "[GIN] 2024/11/20 - 01:11:55 | 200 |  1.646579408s |       127.0.0.1 | POST     \"/api/pull\"\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest\n",
      "pulling ac3d1ba8aa77... 100% ▕████████████████▏  19 GB\n",
      "pulling 66b9ea09bd5b... 100% ▕████████████████▏   68 B\n",
      "pulling e94a8ecb9327... 100% ▕████████████████▏ 1.6 KB\n",
      "pulling 832dd9e00a68... 100% ▕████████████████▏  11 KB\n",
      "pulling f0676bd3c336... 100% ▕████████████████▏  488 B\n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "success \u001b[?25h\n",
      "t=2024-11-20T01:15:15+0000 lvl=info msg=\"join connections\" obj=join id=718eba499c4e l=127.0.0.1:11434 r=72.136.100.33:59359\n",
      "[GIN] 2024/11/20 - 01:15:15 | 200 |      63.669µs |   72.136.100.33 | GET      \"/\"\n",
      "[GIN] 2024/11/20 - 01:15:15 | 404 |       7.996µs |   72.136.100.33 | GET      \"/favicon.ico\"\n",
      "[GIN] 2024/11/20 - 01:15:22 | 404 |        8.79µs |   72.136.100.33 | GET      \"/tags\"\n",
      "[GIN] 2024/11/20 - 01:15:34 | 200 |    1.260149ms |   72.136.100.33 | GET      \"/api/tags\"\n",
      "t=2024-11-20T01:16:32+0000 lvl=info msg=\"join connections\" obj=join id=3da8fb928078 l=127.0.0.1:11434 r=72.136.100.33:59382\n",
      "t=2024-11-20T01:16:32+0000 lvl=info msg=\"join connections\" obj=join id=0bf1ad32998c l=127.0.0.1:11434 r=72.136.100.33:59356\n",
      "t=2024-11-20T01:16:32+0000 lvl=info msg=\"join connections\" obj=join id=0ab70f40de69 l=127.0.0.1:11434 r=72.136.100.33:59341\n",
      "[GIN] 2024/11/20 - 01:16:32 | 200 |      484.48µs |   72.136.100.33 | GET      \"/api/tags\"\n",
      "t=2024-11-20T01:16:33+0000 lvl=info msg=\"join connections\" obj=join id=fa30a511db4c l=127.0.0.1:11434 r=72.136.100.33:59364\n",
      "[GIN] 2024/11/20 - 01:16:33 | 200 |     478.677µs |   72.136.100.33 | GET      \"/api/tags\"\n",
      "t=2024-11-20T01:16:34+0000 lvl=info msg=\"join connections\" obj=join id=c36ed0681bf3 l=127.0.0.1:11434 r=72.136.100.33:59362\n",
      "t=2024-11-20T01:16:34+0000 lvl=info msg=\"join connections\" obj=join id=445dfa9e2f9b l=127.0.0.1:11434 r=72.136.100.33:59373\n",
      "[GIN] 2024/11/20 - 01:16:34 | 200 |     509.696µs |   72.136.100.33 | GET      \"/api/tags\"\n",
      "t=2024-11-20T01:16:36+0000 lvl=info msg=\"join connections\" obj=join id=755a6d0eb93f l=127.0.0.1:11434 r=72.136.100.33:59347\n",
      "t=2024-11-20T01:16:36+0000 lvl=info msg=\"join connections\" obj=join id=4f7a0c14160d l=127.0.0.1:11434 r=72.136.100.33:59365\n",
      "[GIN] 2024/11/20 - 01:16:36 | 200 |      488.36µs |   72.136.100.33 | GET      \"/api/tags\"\n",
      "t=2024-11-20T01:16:38+0000 lvl=info msg=\"join connections\" obj=join id=6cb331b0cfe2 l=127.0.0.1:11434 r=72.136.100.33:59370\n",
      "t=2024-11-20T01:16:38+0000 lvl=info msg=\"join connections\" obj=join id=1073cc35a995 l=127.0.0.1:11434 r=72.136.100.33:59341\n",
      "[GIN] 2024/11/20 - 01:16:38 | 200 |     506.883µs |   72.136.100.33 | GET      \"/api/tags\"\n",
      "t=2024-11-20T01:16:40+0000 lvl=info msg=\"join connections\" obj=join id=808797a75cef l=127.0.0.1:11434 r=72.136.100.33:59342\n",
      "t=2024-11-20T01:16:40+0000 lvl=info msg=\"join connections\" obj=join id=85e15b1b9eaf l=127.0.0.1:11434 r=72.136.100.33:59384\n",
      "[GIN] 2024/11/20 - 01:16:40 | 200 |     458.414µs |   72.136.100.33 | GET      \"/api/tags\"\n",
      "t=2024-11-20T01:16:42+0000 lvl=info msg=\"join connections\" obj=join id=793bf92e5a44 l=127.0.0.1:11434 r=72.136.100.33:59373\n",
      "t=2024-11-20T01:16:43+0000 lvl=info msg=\"join connections\" obj=join id=e13be13c1ad7 l=127.0.0.1:11434 r=72.136.100.33:59356\n",
      "[GIN] 2024/11/20 - 01:16:43 | 200 |     468.598µs |   72.136.100.33 | GET      \"/api/tags\"\n",
      "t=2024-11-20T01:16:44+0000 lvl=info msg=\"join connections\" obj=join id=efd9f6fc0337 l=127.0.0.1:11434 r=72.136.100.33:59362\n",
      "t=2024-11-20T01:16:44+0000 lvl=info msg=\"join connections\" obj=join id=5e2db7bacc8c l=127.0.0.1:11434 r=72.136.100.33:59391\n",
      "[GIN] 2024/11/20 - 01:16:44 | 200 |     475.663µs |   72.136.100.33 | GET      \"/api/tags\"\n",
      "t=2024-11-20T01:17:38+0000 lvl=info msg=\"join connections\" obj=join id=20f37dac6119 l=127.0.0.1:11434 r=72.136.100.33:35532\n",
      "t=2024-11-20T01:17:38+0000 lvl=info msg=\"join connections\" obj=join id=2d3a73b6d14d l=127.0.0.1:11434 r=72.136.100.33:35531\n",
      "[GIN] 2024/11/20 - 01:17:38 | 200 |     515.251µs |   72.136.100.33 | GET      \"/api/tags\"\n",
      "t=2024-11-20T01:19:30+0000 lvl=info msg=\"join connections\" obj=join id=c9ea9ab82812 l=127.0.0.1:11434 r=72.136.100.33:35571\n",
      "t=2024-11-20T01:19:31+0000 lvl=info msg=\"join connections\" obj=join id=b7f504a4ca32 l=127.0.0.1:11434 r=72.136.100.33:35577\n",
      "[GIN] 2024/11/20 - 01:19:31 | 404 |       31.08µs |   72.136.100.33 | GET      \"/api/chat\"\n"
     ]
    }
   ],
   "source": [
    "!curl https://ollama.ai/install.sh | sh\n",
    "\n",
    "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
    "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
    "\n",
    "!pip install pyngrok\n",
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(userdata.get('authtoken'))\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
    "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
    "\n",
    "async def run_process(cmd):\n",
    "  print('>>> starting', *cmd)\n",
    "  p = await asyncio.subprocess.create_subprocess_exec(\n",
    "      *cmd,\n",
    "      stdout=asyncio.subprocess.PIPE,\n",
    "      stderr=asyncio.subprocess.PIPE,\n",
    "  )\n",
    "\n",
    "  async def pipe(lines):\n",
    "    async for line in lines:\n",
    "      print(line.strip().decode('utf-8'))\n",
    "\n",
    "  await asyncio.gather(\n",
    "      pipe(p.stdout),\n",
    "      pipe(p.stderr),\n",
    "  )\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "await asyncio.gather(\n",
    "run_process(['ollama', 'serve']),\n",
    "run_process(['ollama', 'pull', 'qwen2.5-coder:32b']),\n",
    "run_process(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header=\"localhost:11434\"'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCRSbkAK52mg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQh26TW0F2GA"
   },
   "source": [
    "Enjoy!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
