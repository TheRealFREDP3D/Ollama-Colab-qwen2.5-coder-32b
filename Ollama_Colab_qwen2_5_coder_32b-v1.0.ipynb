{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Ollama with Ngrok in Google Colab\n",
    "\n",
    "This notebook sets up an Ollama server with Ngrok tunneling in Google Colab, allowing you to access your Ollama instance from anywhere.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "First, you'll need to configure your ngrok authentication token:\n",
    "\n",
    "1. Go to https://ngrok.com and sign up for a free account\n",
    "2. After signing in, go to https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "3. Copy your authtoken\n",
    "4. In Google Colab:\n",
    "   - Click on the key icon in the left sidebar to open \"Secrets\"\n",
    "   - Click \"Add new secret\"\n",
    "   - Set \"Name\" as: authtoken\n",
    "   - Set \"Value\" as: your-ngrok-token-here\n",
    "   - Click \"Add\"\n",
    "5. Make sure to use a GPU runtime (Runtime -> Change runtime type -> GPU)\n",
    "\n",
    "## 1. Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!curl https://ollama.ai/install.sh | sh\n",
    "\n",
    "# Install CUDA drivers\n",
    "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
    "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
    "\n",
    "# Install pyngrok\n",
    "!pip install pyngrok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies and Configure Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "from google.colab import userdata\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Verify ngrok token exists\n",
    "try:\n",
    "    token = userdata.get('authtoken')\n",
    "    if not token:\n",
    "        raise ValueError(\"No authtoken found in Colab secrets\")\n",
    "    ngrok.set_auth_token(token)\n",
    "except Exception as e:\n",
    "    print(\"ERROR: Could not find ngrok authtoken in Colab secrets!\")\n",
    "    print(\"Please follow the instructions at the top of this notebook to set up your ngrok authtoken\")\n",
    "    print(\"Then restart the runtime and run again\")\n",
    "    raise e\n",
    "\n",
    "# Set LD_LIBRARY_PATH for NVIDIA library\n",
    "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_print_output(process):\n",
    "    \"\"\"Helper function to continuously read and print process output\"\"\"\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start Ollama Server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start ollama serve\n",
    "print('>>> starting ollama serve')\n",
    "ollama_serve = subprocess.Popen(\n",
    "    ['ollama', 'serve'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "# Start thread to print ollama serve output\n",
    "serve_thread = threading.Thread(target=run_and_print_output, args=(ollama_serve,))\n",
    "serve_thread.daemon = True\n",
    "serve_thread.start()\n",
    "\n",
    "# Give ollama serve a moment to start up\n",
    "time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pull Ollama Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the model\n",
    "print('>>> starting ollama pull qwen2.5-coder:32b')\n",
    "# ****************************************************\n",
    "# Change the model name to the one you want to pull\n",
    "# See model library on Ollama website\n",
    "# ****************************************************\n",
    "pull_process = subprocess.Popen(\n",
    "    ['ollama', 'pull', 'qwen2.5-coder:32b'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "# Print pull process output\n",
    "while True:\n",
    "    output = pull_process.stdout.readline()\n",
    "    if output == '' and pull_process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Ngrok Tunnel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start ngrok\n",
    "print('>>> starting ngrok http server')\n",
    "ngrok_process = subprocess.Popen(\n",
    "    ['ngrok', 'http', '--log', 'stderr', '11434', '--host-header=localhost:11434'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "# Start thread to print ngrok output\n",
    "ngrok_thread = threading.Thread(target=run_and_print_output, args=(ngrok_process,))\n",
    "ngrok_thread.daemon = True\n",
    "ngrok_thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Get Public URL and Keep Server Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display the public URL\n",
    "time.sleep(5)  # Wait for ngrok to start\n",
    "try:\n",
    "    tunnels = ngrok.get_tunnels()\n",
    "    if tunnels:\n",
    "        print(\"\\n=== Your Ollama server is available at ===\")\n",
    "        print(tunnels[0].public_url)\n",
    "        print(\"=====================================\")\n",
    "    else:\n",
    "        print(\"No active ngrok tunnels found\")\n",
    "except Exception as e:\n",
    "    print(\"Error getting ngrok URL:\", e)\n",
    "\n",
    "# Keep the main process running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Shutting down...\")\n",
    "    ollama_serve.terminate()\n",
    "    ngrok_process.terminate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "1. Run all cells in order\n",
    "2. Wait for the model to download (this may take a while)\n",
    "3. Once complete, you'll see a public URL where your Ollama server is accessible\n",
    "4. You can now use this URL to connect to your Ollama instance from anywhere\n",
    "\n",
    "Note: The server will keep running until you stop the notebook execution or disconnect from the Colab runtime.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
